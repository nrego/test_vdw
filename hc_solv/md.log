Log file opened on Tue Apr 25 16:31:46 2017
Host: nick-ThinkPad-T440p  pid: 11897  nodeid: 0  nnodes:  1
The Gromacs distribution was built Thu Mar 23 15:14:35 EDT 2017 by
nick@nick-ThinkPad-T440p (Linux 3.19.0-58-generic x86_64)


                         :-)  G  R  O  M  A  C  S  (-:

               Giving Russians Opium May Alter Current Situation

                            :-)  VERSION 4.5.6  (-:

        Written by Emile Apol, Rossen Apostolov, Herman J.C. Berendsen,
      Aldert van Buuren, PÃ¤r Bjelkmar, Rudi van Drunen, Anton Feenstra, 
        Gerrit Groenhof, Peter Kasson, Per Larsson, Pieter Meulenhoff, 
           Teemu Murtola, Szilard Pall, Sander Pronk, Roland Schulz, 
                Michael Shirts, Alfons Sijbers, Peter Tieleman,

               Berk Hess, David van der Spoel, and Erik Lindahl.

       Copyright (c) 1991-2000, University of Groningen, The Netherlands.
            Copyright (c) 2001-2010, The GROMACS development team at
        Uppsala University & The Royal Institute of Technology, Sweden.
            check out http://www.gromacs.org for more information.

         This program is free software; you can redistribute it and/or
          modify it under the terms of the GNU General Public License
         as published by the Free Software Foundation; either version 2
             of the License, or (at your option) any later version.

                                :-)  mdrun  (-:


++++ PLEASE READ AND CITE THE FOLLOWING REFERENCE ++++
B. Hess and C. Kutzner and D. van der Spoel and E. Lindahl
GROMACS 4: Algorithms for highly efficient, load-balanced, and scalable
molecular simulation
J. Chem. Theory Comput. 4 (2008) pp. 435-447
-------- -------- --- Thank You --- -------- --------


++++ PLEASE READ AND CITE THE FOLLOWING REFERENCE ++++
D. van der Spoel, E. Lindahl, B. Hess, G. Groenhof, A. E. Mark and H. J. C.
Berendsen
GROMACS: Fast, Flexible and Free
J. Comp. Chem. 26 (2005) pp. 1701-1719
-------- -------- --- Thank You --- -------- --------


++++ PLEASE READ AND CITE THE FOLLOWING REFERENCE ++++
E. Lindahl and B. Hess and D. van der Spoel
GROMACS 3.0: A package for molecular simulation and trajectory analysis
J. Mol. Mod. 7 (2001) pp. 306-317
-------- -------- --- Thank You --- -------- --------


++++ PLEASE READ AND CITE THE FOLLOWING REFERENCE ++++
H. J. C. Berendsen, D. van der Spoel and R. van Drunen
GROMACS: A message-passing parallel molecular dynamics implementation
Comp. Phys. Comm. 91 (1995) pp. 43-56
-------- -------- --- Thank You --- -------- --------

Input Parameters:
   integrator           = md
   nsteps               = 500
   init_step            = 0
   ns_type              = Grid
   nstlist              = 10
   ndelta               = 2
   nstcomm              = 0
   comm_mode            = None
   nstlog               = 500
   nstxout              = 0
   nstvout              = 0
   nstfout              = 0
   nstcalcenergy        = 10
   nstenergy            = 10
   nstxtcout            = 10
   init_t               = 0
   delta_t              = 0.002
   xtcprec              = 1e+06
   nkx                  = 25
   nky                  = 25
   nkz                  = 25
   pme_order            = 4
   ewald_rtol           = 1e-05
   ewald_geometry       = 0
   epsilon_surface      = 0
   optimize_fft         = FALSE
   ePBC                 = xyz
   bPeriodicMols        = FALSE
   bContinuation        = FALSE
   bShakeSOR            = FALSE
   etc                  = No
   nsttcouple           = -1
   epc                  = No
   epctype              = Isotropic
   nstpcouple           = -1
   tau_p                = 1
   ref_p (3x3):
      ref_p[    0]={ 0.00000e+00,  0.00000e+00,  0.00000e+00}
      ref_p[    1]={ 0.00000e+00,  0.00000e+00,  0.00000e+00}
      ref_p[    2]={ 0.00000e+00,  0.00000e+00,  0.00000e+00}
   compress (3x3):
      compress[    0]={ 0.00000e+00,  0.00000e+00,  0.00000e+00}
      compress[    1]={ 0.00000e+00,  0.00000e+00,  0.00000e+00}
      compress[    2]={ 0.00000e+00,  0.00000e+00,  0.00000e+00}
   refcoord_scaling     = No
   posres_com (3):
      posres_com[0]= 0.00000e+00
      posres_com[1]= 0.00000e+00
      posres_com[2]= 0.00000e+00
   posres_comB (3):
      posres_comB[0]= 0.00000e+00
      posres_comB[1]= 0.00000e+00
      posres_comB[2]= 0.00000e+00
   andersen_seed        = 815131
   rlist                = 1
   rlistlong            = 1
   rtpi                 = 0.05
   coulombtype          = PME
   rcoulomb_switch      = 0
   rcoulomb             = 1
   vdwtype              = Cut-off
   rvdw_switch          = 0
   rvdw                 = 1
   epsilon_r            = 1
   epsilon_rf           = 1
   tabext               = 1
   implicit_solvent     = No
   gb_algorithm         = Still
   gb_epsilon_solvent   = 80
   nstgbradii           = 1
   rgbradii             = 1
   gb_saltconc          = 0
   gb_obc_alpha         = 1
   gb_obc_beta          = 0.8
   gb_obc_gamma         = 4.85
   gb_dielectric_offset = 0.009
   sa_algorithm         = Ace-approximation
   sa_surface_tension   = 2.05016
   DispCorr             = No
   free_energy          = no
   init_lambda          = 0
   delta_lambda         = 0
   n_foreign_lambda     = 0
   sc_alpha             = 0
   sc_power             = 0
   sc_sigma             = 0.3
   sc_sigma_min         = 0.3
   nstdhdl              = 10
   separate_dhdl_file   = yes
   dhdl_derivatives     = yes
   dh_hist_size         = 0
   dh_hist_spacing      = 0.1
   nwall                = 0
   wall_type            = 9-3
   wall_atomtype[0]     = -1
   wall_atomtype[1]     = -1
   wall_density[0]      = 0
   wall_density[1]      = 0
   wall_ewald_zfac      = 3
   pull                 = no
   disre                = No
   disre_weighting      = Conservative
   disre_mixed          = FALSE
   dr_fc                = 1000
   dr_tau               = 0
   nstdisreout          = 100
   orires_fc            = 0
   orires_tau           = 0
   nstorireout          = 100
   dihre-fc             = 1000
   em_stepsize          = 0.01
   em_tol               = 10
   niter                = 20
   fc_stepsize          = 0
   nstcgsteep           = 1000
   nbfgscorr            = 10
   ConstAlg             = Lincs
   shake_tol            = 0.0001
   lincs_order          = 4
   lincs_warnangle      = 30
   lincs_iter           = 1
   bd_fric              = 0
   ld_seed              = 1993
   cos_accel            = 0
   deform (3x3):
      deform[    0]={ 0.00000e+00,  0.00000e+00,  0.00000e+00}
      deform[    1]={ 0.00000e+00,  0.00000e+00,  0.00000e+00}
      deform[    2]={ 0.00000e+00,  0.00000e+00,  0.00000e+00}
   userint1             = 0
   userint2             = 0
   userint3             = 0
   userint4             = 0
   userreal1            = 0
   userreal2            = 0
   userreal3            = 0
   userreal4            = 0
grpopts:
   nrdf:        5373
   ref_t:           0
   tau_t:           0
anneal:          No
ann_npoints:           0
   acc:	           0           0           0
   nfreeze:           N           N           N
   energygrp_flags[  0]: 0
   efield-x:
      n = 0
   efield-xt:
      n = 0
   efield-y:
      n = 0
   efield-yt:
      n = 0
   efield-z:
      n = 0
   efield-zt:
      n = 0
   bQMMM                = FALSE
   QMconstraints        = 0
   QMMMscheme           = 0
   scalefactor          = 1
qm_opts:
   ngQM                 = 0

Initializing Domain Decomposition on 8 nodes
Dynamic load balancing: auto
Will sort the charge groups at every domain (re)decomposition
Using 0 separate PME nodes, as there are too few total
 nodes for efficient splitting
Scaling the initial minimum size with 1/0.8 (option -dds) = 1.25
Optimizing the DD grid for 8 cells with a minimum initial size of 0.000 nm
Domain decomposition grid 4 x 2 x 1, separate PME nodes 0
PME domain decomposition: 4 x 2 x 1
Domain decomposition nodeid 0, coordinates 0 0 0

Table routines are used for coulomb: TRUE
Table routines are used for vdw:     FALSE
Will do PME sum in reciprocal space.

++++ PLEASE READ AND CITE THE FOLLOWING REFERENCE ++++
U. Essmann, L. Perera, M. L. Berkowitz, T. Darden, H. Lee and L. G. Pedersen 
A smooth particle mesh Ewald method
J. Chem. Phys. 103 (1995) pp. 8577-8592
-------- -------- --- Thank You --- -------- --------

Will do ordinary reciprocal space Ewald sum.
Using a Gaussian width (1/beta) of 0.320163 nm for Ewald
Cut-off's:   NS: 1   Coulomb: 1   LJ: 1
System total charge: 0.000
Generated table with 1000 data points for Ewald.
Tabscale = 500 points/nm
Generated table with 1000 data points for LJ6.
Tabscale = 500 points/nm
Generated table with 1000 data points for LJ12.
Tabscale = 500 points/nm

Enabling SPC-like water optimization for 895 molecules.

Configuring nonbonded kernels...
Configuring standard C nonbonded kernels...
Testing x86_64 SSE2 support... present.


Removing pbc first time

++++ PLEASE READ AND CITE THE FOLLOWING REFERENCE ++++
S. Miyamoto and P. A. Kollman
SETTLE: An Analytical Version of the SHAKE and RATTLE Algorithms for Rigid
Water Models
J. Comp. Chem. 13 (1992) pp. 952-962
-------- -------- --- Thank You --- -------- --------


Linking all bonded interactions to atoms

The initial number of communication pulses is: X 2 Y 1
The initial domain decomposition cell size is: X 0.75 nm Y 1.50 nm

When dynamic load balancing gets turned on, these settings will change to:
The maximum number of communication pulses is: X 2 Y 1
The minimum size for domain decomposition cells is 0.500 nm
The requested allowed shrink of DD cells (option -dds) is: 0.80
The allowed shrink of domain decomposition cells is: X 0.67 Y 0.67

Making 2D domain decomposition grid 4 x 2 x 1, home cell index 0 0 0

There are: 2686 Atoms
Charge group distribution at step 0: 118 109 123 103 113 116 114 100
Grid: 5 x 4 x 5 cells

Constraining the starting coordinates (step 0)

Constraining the coordinates at t0-dt (step 0)
RMS relative constraint deviation after constraining: 0.00e+00
Initial temperature: 3.78154e-07 K

Started mdrun on node 0 Tue Apr 25 16:31:46 2017

           Step           Time         Lambda
              0        0.00000        0.00000

   Energies (kJ/mol)
        LJ (SR)   Coulomb (SR)   Coul. recip.      Potential    Kinetic En.
    1.68356e+04   -3.82330e+04   -2.78471e+03   -2.41821e+04    7.28213e+02
   Total Energy    Temperature Pressure (bar)
   -2.34539e+04    3.26013e+01    2.79805e+04

DD  step 9 load imb.: force 30.3%

At step 10 the performance loss due to force load imbalance is 8.6 %

NOTE: Turning on dynamic load balancing

DD  step 499  vol min/aver 0.881  load imb.: force  2.4%

           Step           Time         Lambda
            500        1.00000        0.00000

Writing checkpoint, step 500 at Tue Apr 25 16:31:47 2017


   Energies (kJ/mol)
        LJ (SR)   Coulomb (SR)   Coul. recip.      Potential    Kinetic En.
    6.07700e+03   -3.73479e+04   -3.28166e+03   -3.45525e+04    1.06768e+04
   Total Energy    Temperature Pressure (bar)
   -2.38757e+04    4.77990e+02    2.79170e+03

	<======  ###############  ==>
	<====  A V E R A G E S  ====>
	<==  ###############  ======>

	Statistics over 501 steps using 51 frames

   Energies (kJ/mol)
        LJ (SR)   Coulomb (SR)   Coul. recip.      Potential    Kinetic En.
    7.37004e+03   -3.89422e+04   -3.26457e+03   -3.48367e+04    1.09656e+04
   Total Energy    Temperature Pressure (bar)
   -2.38711e+04    4.90920e+02    5.44225e+03

   Total Virial (kJ/mol)
   -7.89645e+02   -2.84831e+01    5.43446e+01
   -2.80397e+01   -6.82629e+02    1.41447e+02
    5.52953e+01    1.42775e+02   -8.35543e+02

   Pressure (bar)
    5.60782e+03    2.55288e+01   -4.10072e+01
    2.49834e+01    5.33862e+03   -2.00923e+02
   -4.21766e+01   -2.02555e+02    5.38031e+03

   Total Dipole (D)
    3.92430e+01   -4.53683e+01    1.50110e+01


	M E G A - F L O P S   A C C O U N T I N G

   RF=Reaction-Field  FE=Free Energy  SCFE=Soft-Core/Free Energy
   T=Tabulated        W3=SPC/TIP3p    W4=TIP4p (single or pairs)
   NF=No Forces

 Computing:                               M-Number         M-Flops  % Flops
-----------------------------------------------------------------------------
 Coul(T) + LJ [W3-W3]                    31.167830       11906.111    78.3
 Outer nonbonded loop                     8.762148          87.621     0.6
 Calc Weights                             4.037058         145.334     1.0
 Spread Q Bspline                        86.123904         172.248     1.1
 Gather F Bspline                        86.123904         516.743     3.4
 3D-FFT                                 218.115360        1744.923    11.5
 Solve PME                                4.070625         260.520     1.7
 NS-Pairs                                 8.648400         181.616     1.2
 Reset In Box                             0.045696           0.137     0.0
 CG-CoM                                   0.139672           0.419     0.0
 Virial                                   0.155346           2.796     0.0
 Calc-Ekin                                1.348372          36.406     0.2
 Constraint-V                             1.347870          10.783     0.1
 Constraint-Vir                           0.136935           3.286     0.0
 Settle                                   0.450185         145.410     1.0
-----------------------------------------------------------------------------
 Total                                                   15214.355   100.0
-----------------------------------------------------------------------------


    D O M A I N   D E C O M P O S I T I O N   S T A T I S T I C S

 av. #atoms communicated per step for force:  2 x 7306.6

 Average load imbalance: 5.9 %
 Part of the total run time spent waiting due to load imbalance: 2.5 %
 Steps where the load balancing was limited by -rdd, -rcon and/or -dds: X 0 % Y 0 %


     R E A L   C Y C L E   A N D   T I M E   A C C O U N T I N G

 Computing:         Nodes     Number     G-Cycles    Seconds     %
-----------------------------------------------------------------------
 Domain decomp.         8         51        0.139        0.1     0.9
 DD comm. load          8         50        0.004        0.0     0.0
 DD comm. bounds        8         50        0.014        0.0     0.1
 Comm. coord.           8        501        0.296        0.1     1.9
 Neighbor search        8         51        0.662        0.2     4.3
 Force                  8        501        6.674        2.5    43.2
 Wait + Comm. F         8        501        0.302        0.1     2.0
 PME mesh               8        501        4.618        1.7    29.9
 Write traj.            8         51        0.232        0.1     1.5
 Update                 8        501        0.187        0.1     1.2
 Constraints            8        501        0.290        0.1     1.9
 Comm. energies         8         51        1.921        0.7    12.4
 Rest                   8                   0.113        0.0     0.7
-----------------------------------------------------------------------
 Total                  8                  15.454        5.7   100.0
-----------------------------------------------------------------------
-----------------------------------------------------------------------
 PME redist. X/F        8       1002        1.881        0.7    12.2
 PME spread/gather      8       1002        1.724        0.6    11.2
 PME 3D-FFT             8       1002        0.847        0.3     5.5
 PME solve              8        501        0.160        0.1     1.0
-----------------------------------------------------------------------

NOTE: 12 % of the run time was spent communicating energies,
      you might want to use the -gcom option of mdrun


	Parallel run - timing based on wallclock.

               NODE (s)   Real (s)      (%)
       Time:      0.717      0.717    100.0
               (Mnbf/s)   (GFlops)   (ns/day)  (hour/ns)
Performance:    391.087     21.212    120.700      0.199
Finished mdrun on node 0 Tue Apr 25 16:31:47 2017
